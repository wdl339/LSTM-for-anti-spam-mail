{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21aafd25",
   "metadata": {},
   "source": [
    "## 构建LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ab8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import tqdm\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from random import *\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 320\n",
    "src_vocab_size = 21128          # 字典大小\n",
    "d_model = 512                   # embedding维度\n",
    "num_layers = 2                  # LSTM层数\n",
    "hidden_size = 100               # LSTM隐藏层\n",
    "linear_hidden_size = 10         # 全链接层隐藏数\n",
    "classes = 2                     # 类别数\n",
    "dropout = 0.3                   # LSTM中dropout\n",
    "lr = 1e-3                       # 学习率\n",
    "epochs = 10                     # 训练次数    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95aca884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21128it [00:00, 1344865.99it/s]\n",
      "5978it [00:00, 61429.18it/s]\n",
      "21128it [00:00, 1472463.24it/s]\n",
      "800000it [00:12, 62562.96it/s]\n",
      "21128it [00:00, 1564376.84it/s]\n",
      "2000it [00:00, 76442.14it/s]\n",
      "21128it [00:00, 1759955.01it/s]\n",
      "1993it [00:00, 77312.39it/s]\n",
      "21128it [00:00, 1787142.64it/s]\n",
      "9985it [00:00, 50810.78it/s]\n",
      "21128it [00:00, 1749496.67it/s]\n",
      "1999it [00:00, 50166.72it/s]\n",
      "21128it [00:00, 1755701.05it/s]\n",
      "1997it [00:00, 52485.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "815963\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(path, pad_size=100):\n",
    "    contents = []\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    with open(\"./vocab.txt\", 'r', encoding='UTF-8') as f:\n",
    "        idx2word = {idx: line.strip() for idx, line in  enumerate(tqdm.tqdm(f))}\n",
    "        word2idx = {idx2word[key]: key for key in  idx2word}\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        for line in tqdm.tqdm(f):\n",
    "            token_ids = []\n",
    "            lin = line.strip()\n",
    "            if not lin:\n",
    "                continue\n",
    "            label = lin[0]\n",
    "            content = lin[2:]\n",
    "            token_ids.append (word2idx['[CLS]'])\n",
    "            for key in content:\n",
    "                token_ids.append(word2idx.get(key, 0))\n",
    "            seq_len = len(token_ids)\n",
    "            mask = []\n",
    "            if pad_size:\n",
    "                if seq_len < pad_size:\n",
    "                    token_ids += ([0] * (pad_size - seq_len))\n",
    "                else:\n",
    "                    token_ids = token_ids[:pad_size]\n",
    "                    seq_len = pad_size\n",
    "            contents.append((numpy.array(token_ids), int(label), seq_len))\n",
    "    return contents\n",
    "def load_dataset_backward(path, pad_size=100):\n",
    "    contents = []\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    with open(\"./vocab.txt\", 'r', encoding='UTF-8') as f:\n",
    "        idx2word = {idx: line.strip() for idx, line in  enumerate(tqdm.tqdm(f))}\n",
    "        word2idx = {idx2word[key]: key for key in  idx2word}\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        for line in tqdm.tqdm(f):\n",
    "            token_ids = []\n",
    "            lin = line.strip()\n",
    "            if not lin:\n",
    "                continue\n",
    "            label = lin[-1]\n",
    "            content = lin[0:-2]\n",
    "            token_ids.append (word2idx['[CLS]'])\n",
    "            for key in content:\n",
    "                token_ids.append(word2idx.get(key, 0))\n",
    "            seq_len = len(token_ids)\n",
    "            mask = []\n",
    "            if pad_size:\n",
    "                if seq_len < pad_size:\n",
    "                    token_ids += ([0] * (pad_size - seq_len))\n",
    "                else:\n",
    "                    token_ids = token_ids[:pad_size]\n",
    "                    seq_len = pad_size\n",
    "            contents.append((numpy.array(token_ids), int(label), seq_len))\n",
    "    return contents\n",
    "\n",
    "train = load_dataset(\"./train.csv\")\n",
    "train += load_dataset(\"./train_aaa.csv\")\n",
    "dev = load_dataset(\"./dev.csv\")\n",
    "test = load_dataset(\"./test.csv\")\n",
    "train += load_dataset_backward(\"./train_large.csv\")\n",
    "dev += load_dataset_backward(\"./validation_set.csv\")\n",
    "test += load_dataset_backward(\"./test_set.csv\")\n",
    "print(len(train))\n",
    "train_input_ids, train_label, seq_len = zip(*train)\n",
    "dev_input_ids, dev_label, seq_len = zip(*dev)\n",
    "test_input_ids, test_label, seq_len = zip(*test)\n",
    "\n",
    "train_input_ids, train_label = torch.tensor(train_input_ids), torch.tensor(train_label)\n",
    "dev_input_ids, dev_label = torch.tensor(dev_input_ids), torch.tensor(dev_label)\n",
    "test_input_ids, test_label = torch.tensor(test_input_ids), torch.tensor(test_label)\n",
    "\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self, input_ids, label):\n",
    "        self.input_ids = input_ids\n",
    "        self.label = label\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.label[idx]\n",
    "\n",
    "train_loader = Data.DataLoader(MyDataSet(train_input_ids, train_label), batch_size, True)\n",
    "dev_loader = Data.DataLoader(MyDataSet(dev_input_ids, dev_label), batch_size, True)\n",
    "test_loader = Data.DataLoader(MyDataSet(test_input_ids, test_label), batch_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8add2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myNet, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(src_vocab_size, d_model)                        \n",
    "        self.lstm = torch.nn.LSTM(input_size=d_model, hidden_size=hidden_size,num_layers=num_layers,dropout=dropout)\n",
    "        self.linear = torch.nn.Linear(hidden_size, linear_hidden_size)\n",
    "        self.linear1 = torch.nn.Linear(linear_hidden_size, classes)\n",
    "    def forward(self,data):\n",
    "        x = self.embed(data)                             # [64,32,512]   把字用字向量表示\n",
    "        x,(h_n, c_n) = self.lstm(x.transpose(0, 1))      # x:[32, 64, 100]    记录每时刻最后一层的输出。\n",
    "                                                         # h_n: [2, 64, 100]  记录每一层最后一次的输出\n",
    "                                                         # c_n: [2, 64, 100]  记录每一层cell保存的值\n",
    "        x = self.linear(x[-1])                           # [64, 10] 经过第一层全连接层\n",
    "        x = self.linear1(x)                              # [64, 2] 经过第二层全连接层\n",
    "        return x       \n",
    "\n",
    "model = myNet().to(device)                              # 定义模型\n",
    "criterion = torch.nn.CrossEntropyLoss()                  # 定义损失函数\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)   # 定义梯度优化算法                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beefc1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, trin_loss:0.011283075922348229\n",
      "epoch:1, ACC:0.9147287011146545\n",
      "epoch:2, trin_loss:0.008165352075853095\n",
      "epoch:2, ACC:0.9109777808189392\n",
      "epoch:3, trin_loss:0.0060553966318207635\n",
      "epoch:3, ACC:0.9242311120033264\n",
      "epoch:4, trin_loss:0.004478563706568639\n",
      "epoch:4, ACC:0.9287322163581848\n",
      "epoch:5, trin_loss:0.0036063570159236766\n",
      "epoch:5, ACC:0.9274818897247314\n",
      "epoch:6, trin_loss:0.0028255865198441306\n",
      "epoch:6, ACC:0.9262316226959229\n",
      "epoch:7, trin_loss:0.0024332802720970926\n",
      "epoch:7, ACC:0.9289822578430176\n",
      "epoch:8, trin_loss:0.002091772300498737\n",
      "epoch:8, ACC:0.9279820322990417\n",
      "epoch:9, trin_loss:0.0018362991670523225\n",
      "epoch:9, ACC:0.9307327270507812\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss_sum = 0.\n",
    "    num = 1\n",
    "    for data, label in train_loader:                     # 遍历一个opoch的数据\n",
    "        data, label = data.to(device) ,label.to(device)  # 加载到GCP上\n",
    "        train_output = model(data)                       # 前向传播\n",
    "        train_loss = criterion(train_output, label)      # 计算损失\n",
    "        train_loss_sum += train_loss.item()\n",
    "        num += 1\n",
    "        optimizer.zero_grad()                            # 梯度清零\n",
    "        train_loss.backward()                            # 反向传播计算梯度\n",
    "        optimizer.step()                                 # 更新梯度\n",
    "    print(f'epoch:{epoch}, trin_loss:{train_loss_sum/num}')\n",
    "    acc_count = 0\n",
    "    dev_len = 0\n",
    "    for data, label in dev_loader:\n",
    "        data, label = data.to(device) ,label.to(device)\n",
    "        dev_output = model(data)\n",
    "        acc_count += torch.sum(dev_output.argmax(dim=1)==label)  # 计算准确个数\n",
    "        dev_len +=len(label)                                      # 计算一共有多少个验证数据\n",
    "    print(f'epoch:{epoch}, ACC:{acc_count/float(dev_len)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f53c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 4692, 4692,  ...,    0,    0,    0],\n",
      "        [ 101, 2207, 7987,  ...,    0,    0,    0],\n",
      "        [ 101, 4692,    0,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 5799,  118,  ...,    0,    0,    0],\n",
      "        [ 101, 5401, 4255,  ...,    0,    0,    0],\n",
      "        [ 101, 4696, 4638,  ...,    0,    0,    0]])\n",
      "tensor([0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101,  852, 3221,  ...,    0,    0,    0],\n",
      "        [ 101,  523, 2544,  ..., 6887,  150,  162],\n",
      "        [ 101, 1166,  744,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1059, 5381,  ..., 1059, 4635, 6598],\n",
      "        [ 101, 4385, 1762,  ...,    0,    0,    0],\n",
      "        [ 101, 3297, 3173,  ...,    0,    0,    0]])\n",
      "tensor([0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 1], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101,  800, 6821,  ...,    0,    0,    0],\n",
      "        [ 101, 6820, 1762,  ...,    0,    0,    0],\n",
      "        [ 101, 6929,  702,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 3616, 5401,  ...,    0,    0,    0],\n",
      "        [ 101, 6820,  833,  ...,    0,    0,    0],\n",
      "        [ 101,    0,  122,  ...,    0,    0,    0]])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 0, 1], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101,  122,  119,  ...,    0,    0,    0],\n",
      "        [ 101,  122,  123,  ...,    0,    0,    0],\n",
      "        [ 101,  523,  676,  ..., 3837, 3717, 6572],\n",
      "        ...,\n",
      "        [ 101,  782, 2157,  ...,    0,    0,    0],\n",
      "        [ 101, 5838, 2526,  ...,    0,    0,    0],\n",
      "        [ 101, 6821, 3416,  ...,    0,    0,    0]])\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101, 2849, 3624,  ...,    0,    0,    0],\n",
      "        [ 101,  686, 4518,  ...,    0,    0,    0],\n",
      "        [ 101, 6432, 7509,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  120,  146,  ...,    0,    0,    0],\n",
      "        [ 101, 2141, 1213,  ...,    0,    0,    0],\n",
      "        [ 101, 2228, 7574,  ...,    0,    0,    0]])\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101, 2512,    0,  ...,    0,    0,    0],\n",
      "        [ 101,  125,  129,  ...,    0,    0,    0],\n",
      "        [ 101, 7309, 7579,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,    0, 7607,  ...,    0,    0,    0],\n",
      "        [ 101, 3300,  763,  ...,    0,    0,    0],\n",
      "        [ 101, 1309, 3553,  ...,    0,    0,    0]])\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101,  872,  812,  ...,    0,    0,    0],\n",
      "        [ 101, 1962, 1435,  ...,    0,    0,    0],\n",
      "        [ 101, 2769, 6206,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1383,    0,  ...,    0,    0,    0],\n",
      "        [ 101,  704, 4999,  ...,    0,    0,    0],\n",
      "        [ 101, 1343, 7309,  ...,    0,    0,    0]])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101,    0, 1409,  ...,    0,    0,    0],\n",
      "        [ 101,  147,  158,  ...,    0,    0,    0],\n",
      "        [ 101, 4668,    0,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  164,  151,  ...,    0,    0,    0],\n",
      "        [ 101,  676, 1744,  ...,    0,    0,    0],\n",
      "        [ 101,  129,  128,  ...,    0,    0,    0]])\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101, 6651, 6756,  ...,    0,    0,    0],\n",
      "        [ 101,  126,  121,  ...,    0,    0,    0],\n",
      "        [ 101, 4155, 1084,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 7577,    0,  ...,    0,    0,    0],\n",
      "        [ 101, 1638, 8013,  ...,    0,    0,    0],\n",
      "        [ 101, 1920,  830,  ...,    0,    0,    0]])\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101, 1343, 4381,  ...,    0,    0,    0],\n",
      "        [ 101,  128,    0,  ...,    0,    0,    0],\n",
      "        [ 101, 5632, 2346,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,    0, 4125,  ...,    0,    0,    0],\n",
      "        [ 101, 2769, 6820,  ...,    0,    0,    0],\n",
      "        [ 101, 5709, 6486,  ...,    0,    0,    0]])\n",
      "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101, 1184, 3667,  ...,    0,    0,    0],\n",
      "        [ 101, 6929, 4905,  ...,    0,    0,    0],\n",
      "        [ 101,  165,  147,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  113,    0,  ...,    0,    0,    0],\n",
      "        [ 101,    0,  130,  ...,    0,    0,    0],\n",
      "        [ 101,  143,  167,  ...,    0,    0,    0]])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 1], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101,  153,  163,  ...,    0,    0,    0],\n",
      "        [ 101, 3187, 7444,  ...,    0,    0,    0],\n",
      "        [ 101, 2697, 6230,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  115, 7028,  ...,    0,    0,    0],\n",
      "        [ 101,  872, 2556,  ...,    0,    0,    0],\n",
      "        [ 101,    0,    0,  ...,    0,    0,    0]])\n",
      "tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 1], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[ 101, 3680, 1921,  ...,  162,  119,  155],\n",
      "        [ 101,    0,    0,  ...,    0,    0,    0],\n",
      "        [ 101, 3381, 4638,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 3130, 6206,  ...,    0,    0,    0],\n",
      "        [ 101, 1103, 1103,  ...,    0,    0,    0],\n",
      "        [ 101, 1962, 1136,  ...,    0,    0,    0]])\n",
      "tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor(0.9378, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "acc_count = 0\n",
    "test_len = 0\n",
    "for data, label in test_loader:\n",
    "    print(data)# 遍历测试数据\n",
    "    data, label = data.to(device), label.to(device)  # 数据加载到GPU上\n",
    "    test_output = model(data)     \n",
    "    print(test_output.argmax(dim=1))# 预测\n",
    "    acc_count += torch.sum(test_output.argmax(dim=1) == label)  # 计算准确个数\n",
    "    test_len += len(label)                           # 计算一共有多少个测试数据\n",
    "print(acc_count / float(test_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "288faa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21128it [00:00, 1509252.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 5439, 2360, 1962, 8024,  791, 1921, 3300, 4958, 1408,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_size = 60\n",
    "with open(\"./vocab.txt\", 'r', encoding='UTF-8') as f:\n",
    "        idx2word = {idx: line.strip() for idx, line in  enumerate(tqdm.tqdm(f))}\n",
    "        word2idx = {idx2word[key]: key for key in  idx2word}\n",
    "content = '老师好，今天有空吗'\n",
    "token_ids = []\n",
    "token_ids.append (word2idx['[CLS]'])\n",
    "for key in content:\n",
    "    token_ids.append(word2idx.get(key, 0))\n",
    "seq_len = len(token_ids)\n",
    "mask = []\n",
    "if pad_size:\n",
    "    if seq_len < pad_size:\n",
    "        token_ids += ([0] * (pad_size - seq_len))\n",
    "    else:\n",
    "        token_ids = token_ids[:pad_size]\n",
    "        seq_len = pad_size\n",
    "content\n",
    "token_ids\n",
    "datain = torch.tensor(token_ids)\n",
    "datain = datain.unsqueeze(0)\n",
    "datain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672ad2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1], device='cuda:0', grad_fn=<NotImplemented>)\n",
      "tensor([[-2.7726,  0.4816]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_output = model(datain.to(device)) \n",
    "test_output\n",
    "print(test_output.argmax(dim=1))\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d022d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
