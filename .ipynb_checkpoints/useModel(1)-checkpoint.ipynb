{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1078f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import tqdm\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from random import *\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "src_vocab_size = 21128          # 字典大小\n",
    "d_model = 512                   # embedding维度\n",
    "num_layers = 2                  # LSTM层数\n",
    "hidden_size = 100               # LSTM隐藏层\n",
    "linear_hidden_size = 10         # 全链接层隐藏数\n",
    "classes = 2                     # 类别数\n",
    "dropout = 0.3                   # LSTM中dropout\n",
    "lr = 1e-3                       # 学习率\n",
    "epochs = 10                     # 训练次数   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f915ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myNet, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(src_vocab_size, d_model)                        \n",
    "        self.lstm = torch.nn.LSTM(input_size=d_model, hidden_size=hidden_size,num_layers=num_layers,dropout=dropout)\n",
    "        self.linear = torch.nn.Linear(hidden_size, linear_hidden_size)\n",
    "        self.linear1 = torch.nn.Linear(linear_hidden_size, classes)\n",
    "    def forward(self,data):\n",
    "        x = self.embed(data)                             # [64,32,512]   把字用字向量表示\n",
    "        x,(h_n, c_n) = self.lstm(x.transpose(0, 1))      # x:[32, 64, 100]    记录每时刻最后一层的输出。\n",
    "                                                         # h_n: [2, 64, 100]  记录每一层最后一次的输出\n",
    "                                                         # c_n: [2, 64, 100]  记录每一层cell保存的值\n",
    "        x = self.linear(x[-1])                           # [64, 10] 经过第一层全连接层\n",
    "        x = self.linear1(x)                              # [64, 2] 经过第二层全连接层\n",
    "        return x       \n",
    "\n",
    "model = myNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9016f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight : torch.Size([21128, 512])\n",
      "lstm.weight_ih_l0 : torch.Size([400, 512])\n",
      "lstm.weight_hh_l0 : torch.Size([400, 100])\n",
      "lstm.bias_ih_l0 : torch.Size([400])\n",
      "lstm.bias_hh_l0 : torch.Size([400])\n",
      "lstm.weight_ih_l1 : torch.Size([400, 100])\n",
      "lstm.weight_hh_l1 : torch.Size([400, 100])\n",
      "lstm.bias_ih_l1 : torch.Size([400])\n",
      "lstm.bias_hh_l1 : torch.Size([400])\n",
      "linear.weight : torch.Size([10, 100])\n",
      "linear.bias : torch.Size([10])\n",
      "linear1.weight : torch.Size([2, 10])\n",
      "linear1.bias : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt', map_location=torch.device('cpu')))\n",
    "for name, parameters in model.named_parameters():\n",
    "    print(name, ':', parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae203119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "ov_model = ov.convert_model('lstm_model.onnx')\n",
    "ov.save_model(ov_model, 'model.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cddd0b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "# 实例化Core对象\n",
    "core = Core() \n",
    "det_ov_model = core.read_model('model.xml')\n",
    "    \n",
    "net = core.compile_model(det_ov_model, device_name=\"AUTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d84b760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21128it [00:00, 1513660.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 101,  113, 7270, 3309, 6411,  928, 1762, 3315, 2356,  868, 1392, 5102,\n",
       "         6598, 3419, 5466, 4917,  809, 1350, 1313, 4995,  510, 4277, 5023,  511,\n",
       "         4510, 6413, 8038,  122,  122,  125,  126,  122,  125,  122,  130,  122,\n",
       "          130,  129,    0, 3330,  836,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_size = 60\n",
    "with open(\"./vocab.txt\", 'r', encoding='UTF-8') as f:\n",
    "        idx2word = {idx: line.strip() for idx, line in  enumerate(tqdm.tqdm(f))}\n",
    "        word2idx = {idx2word[key]: key for key in  idx2word}\n",
    "content = '(长期诚信在本市作各类资格职称以及印章、牌等。电话：11451419198 李伟'\n",
    "token_ids = []\n",
    "token_ids.append (word2idx['[CLS]'])\n",
    "for key in content:\n",
    "    token_ids.append(word2idx.get(key, 0))\n",
    "seq_len = len(token_ids)\n",
    "mask = []\n",
    "if pad_size:\n",
    "    if seq_len < pad_size:\n",
    "        token_ids += ([0] * (pad_size - seq_len))\n",
    "    else:\n",
    "        token_ids = token_ids[:pad_size]\n",
    "        seq_len = pad_size\n",
    "content\n",
    "token_ids\n",
    "datain = torch.tensor(token_ids)\n",
    "datain = datain.unsqueeze(0)\n",
    "datain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a1bc8d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([[0.0863, 0.0268]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time_torch = time.time()\n",
    "test_output = model(datain.to(device)) \n",
    "end_time_torch = time.time()\n",
    "\n",
    "inference_time_torch = end_time_torch - start_time_torch\n",
    "inference_speed_torch = 1 / inference_time_torch\n",
    "\n",
    "print(test_output.argmax(dim=1))\n",
    "print(test_output)\n",
    "\n",
    "print(\"使用PyTorch推理时间：\", inference_time_torch, \"秒\")\n",
    "print(\"使用PyTorch推理速度：\", inference_speed_torch, \"次/秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2d505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(msg):\n",
    "    from openvino.runtime import Core\n",
    "    # 实例化Core对象\n",
    "    core = Core() \n",
    "    det_ov_model = core.read_model('model.xml')\n",
    "        \n",
    "    net = core.compile_model(det_ov_model, device_name=\"AUTO\")\n",
    "    pad_size = 60\n",
    "    with open(\"./vocab.txt\", 'r', encoding='UTF-8') as f:\n",
    "            idx2word = {idx: line.strip() for idx, line in  enumerate(tqdm.tqdm(f))}\n",
    "            word2idx = {idx2word[key]: key for key in  idx2word}\n",
    "    content = str(msg)\n",
    "    token_ids = []\n",
    "    token_ids.append (word2idx['[CLS]'])\n",
    "    for key in content:\n",
    "        token_ids.append(word2idx.get(key, 0))\n",
    "    seq_len = len(token_ids)\n",
    "    mask = []\n",
    "    if pad_size:\n",
    "        if seq_len < pad_size:\n",
    "            token_ids += ([0] * (pad_size - seq_len))\n",
    "        else:\n",
    "            token_ids = token_ids[:pad_size]\n",
    "            seq_len = pad_size\n",
    "    content\n",
    "    token_ids\n",
    "    datain = torch.tensor(token_ids)\n",
    "    datain = datain.unsqueeze(0)\n",
    "    output_node = net.outputs[0] \n",
    "    ir = net.create_infer_request()\n",
    "    outputs = ir.infer(datain)[output_node]\n",
    "    if outputs.argmax() == 1:\n",
    "        return \"这是垃圾内容\"\n",
    "    else:\n",
    "        return \"这是安全内容\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "29737819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "output_node = net.outputs[0] \n",
    "ir = net.create_infer_request()\n",
    "\n",
    "start_time_openvino = time.time()\n",
    "outputs = ir.infer(datain)[output_node]\n",
    "end_time_openvino = time.time()\n",
    "\n",
    "inference_time_openvino = end_time_openvino - start_time_openvino\n",
    "inference_speed_openvino = 1 / inference_time_openvino\n",
    "\n",
    "print(outputs.argmax())\n",
    "print(\"使用OpenVINO推理时间：\", inference_time_openvino, \"秒\")\n",
    "print(\"使用OpenVINO推理速度：\", inference_speed_openvino, \"次/秒\")\n",
    "print(outputs.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eeda9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [25/Dec/2023 10:44:34] \"GET / HTTP/1.1\" 200 -\n",
      "21128it [00:00, 635235.48it/s]\n",
      "127.0.0.1 - - [25/Dec/2023 10:44:38] \"GET /get?msg=这是一个人 HTTP/1.1\" 200 -\n",
      "21128it [00:00, 782333.43it/s]\n",
      "127.0.0.1 - - [25/Dec/2023 10:44:52] \"GET /get?msg=(长期诚信在本市作各类资格职称以及印章、牌等。电话：11451419198%20李伟 HTTP/1.1\" 200 -\n",
      "21128it [00:00, 906950.79it/s]\n",
      "127.0.0.1 - - [25/Dec/2023 10:46:35] \"GET /get?msg=ERROR%3A%20pip's%20dependency%20resolver%20does%20not%20currently%20take%20into%20account%20all%20the%20packages%20that%20are%20installed.%20This%20behaviour%20is%20the%20source%20of%20the%20following%20dependency%20conflicts.%20tensorboard%202.10.0%20requires%20protobuf%3C3.20,%3E%3D3.9.2,%20but%20you%20have%20protobuf%203.20.3%20which%20is%20incompatible.%20ERROR%3A%20pip's%20dependency%20resolver%20does%20not%20currently%20take%20into%20account%20all%20the%20packages%20that%20are%20installed.%20This%20behaviour%20is%20the%20source%20of%20the%20following%20dependency%20conflicts.%20tensorboard%202.10.0%20requires%20protobuf%3C3.20,%3E%3D3.9.2,%20but%20you%20have%20protobuf%203.20.3%20which%20is%20incompatible. HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import tqdm\n",
    "import copy\n",
    "import math\n",
    "import tqdm\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from random import *\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():    \n",
    "    return render_template(\"./index.html\")\n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():    \n",
    "    userText = request.args.get('msg')  \n",
    "    response = str(get_response(userText))\n",
    "    return response\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101a056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21128it [00:00, 618978.22it/s]\n",
      "21128it [00:00, 502311.29it/s]\n",
      "21128it [00:00, 572577.55it/s]\n",
      "21128it [00:00, 750917.32it/s]\n",
      "21128it [00:00, 631280.44it/s]\n",
      "21128it [00:00, 653562.96it/s]\n",
      "21128it [00:00, 658913.34it/s]\n",
      "21128it [00:00, 688257.28it/s]\n",
      "21128it [00:00, 607529.26it/s]\n",
      "21128it [00:00, 716301.62it/s]\n",
      "21128it [00:00, 763185.25it/s]\n",
      "21128it [00:00, 634621.34it/s]\n",
      "21128it [00:00, 665914.63it/s]\n",
      "21128it [00:00, 775385.47it/s]\n",
      "21128it [00:00, 615371.96it/s]\n",
      "21128it [00:00, 584862.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import tqdm\n",
    "import copy\n",
    "import math\n",
    "import tqdm\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from random import *\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # 信息内容检测器\n",
    "    在输入块中键入内容查看结果！\n",
    "    \"\"\")\n",
    "    name = gr.Textbox(label=\"输入\")\n",
    "    output = gr.Textbox(label=\"判断\")\n",
    "    submit_btn = gr.Button(\"提交\")\n",
    "    submit_btn.click(fn=get_response, inputs=name, outputs=output, api_name=\"get_response\")\n",
    "demo.launch()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
