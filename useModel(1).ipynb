{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1078f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import tqdm\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from random import *\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "src_vocab_size = 21128          # 字典大小\n",
    "d_model = 512                   # embedding维度\n",
    "num_layers = 2                  # LSTM层数\n",
    "hidden_size = 100               # LSTM隐藏层\n",
    "linear_hidden_size = 10         # 全链接层隐藏数\n",
    "classes = 2                     # 类别数\n",
    "dropout = 0.3                   # LSTM中dropout\n",
    "lr = 1e-3                       # 学习率\n",
    "epochs = 10                     # 训练次数   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f915ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myNet, self).__init__()\n",
    "        self.embed = torch.nn.Embedding(src_vocab_size, d_model)                        \n",
    "        self.lstm = torch.nn.LSTM(input_size=d_model, hidden_size=hidden_size,num_layers=num_layers,dropout=dropout)\n",
    "        self.linear = torch.nn.Linear(hidden_size, linear_hidden_size)\n",
    "        self.linear1 = torch.nn.Linear(linear_hidden_size, classes)\n",
    "    def forward(self,data):\n",
    "        x = self.embed(data)                             # [64,32,512]   把字用字向量表示\n",
    "        x,(h_n, c_n) = self.lstm(x.transpose(0, 1))      # x:[32, 64, 100]    记录每时刻最后一层的输出。\n",
    "                                                         # h_n: [2, 64, 100]  记录每一层最后一次的输出\n",
    "                                                         # c_n: [2, 64, 100]  记录每一层cell保存的值\n",
    "        x = self.linear(x[-1])                           # [64, 10] 经过第一层全连接层\n",
    "        x = self.linear1(x)                              # [64, 2] 经过第二层全连接层\n",
    "        return x       \n",
    "\n",
    "model = myNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9016f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.weight : torch.Size([21128, 512])\n",
      "lstm.weight_ih_l0 : torch.Size([400, 512])\n",
      "lstm.weight_hh_l0 : torch.Size([400, 100])\n",
      "lstm.bias_ih_l0 : torch.Size([400])\n",
      "lstm.bias_hh_l0 : torch.Size([400])\n",
      "lstm.weight_ih_l1 : torch.Size([400, 100])\n",
      "lstm.weight_hh_l1 : torch.Size([400, 100])\n",
      "lstm.bias_ih_l1 : torch.Size([400])\n",
      "lstm.bias_hh_l1 : torch.Size([400])\n",
      "linear.weight : torch.Size([10, 100])\n",
      "linear.bias : torch.Size([10])\n",
      "linear1.weight : torch.Size([2, 10])\n",
      "linear1.bias : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt', map_location=torch.device('cpu')))\n",
    "for name, parameters in model.named_parameters():\n",
    "    print(name, ':', parameters.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae203119",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openvino'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Windows\\Temp\\ipykernel_17268\\1937582540.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mopenvino\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mov_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lstm_model.onnx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mov_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openvino'"
     ]
    }
   ],
   "source": [
    "import openvino as ov\n",
    "ov_model = ov.convert_model('lstm_model.onnx')\n",
    "ov.save_model(ov_model, 'model.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cddd0b14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openvino'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Windows\\Temp\\ipykernel_17268\\3590083184.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mopenvino\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mruntime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# 实例化Core对象\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdet_ov_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openvino'"
     ]
    }
   ],
   "source": [
    "from openvino.runtime import Core\n",
    "# 实例化Core对象\n",
    "core = Core() \n",
    "det_ov_model = core.read_model('model.xml')\n",
    "    \n",
    "net = core.compile_model(det_ov_model, device_name=\"AUTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d84b760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21128it [00:00, 3638714.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 101,  113, 7270, 3309, 6411,  928, 1762, 3315, 2356,  868, 1392, 5102,\n",
       "         6598, 3419, 5466, 4917,  809, 1350, 1313, 4995,  510, 4277, 5023,  511,\n",
       "         4510, 6413, 8038,  122,  122,  125,  126,  122,  125,  122,  130,  122,\n",
       "          130,  129,    0, 3330,  836,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_size = 60\n",
    "with open(\"./vocab.txt\", 'r', encoding='UTF-8') as f:\n",
    "        idx2word = {idx: line.strip() for idx, line in  enumerate(tqdm.tqdm(f))}\n",
    "        word2idx = {idx2word[key]: key for key in  idx2word}\n",
    "content = '(长期诚信在本市作各类资格职称以及印章、牌等。电话：11451419198 李伟'\n",
    "token_ids = []\n",
    "token_ids.append (word2idx['[CLS]'])\n",
    "for key in content:\n",
    "    token_ids.append(word2idx.get(key, 0))\n",
    "seq_len = len(token_ids)\n",
    "mask = []\n",
    "if pad_size:\n",
    "    if seq_len < pad_size:\n",
    "        token_ids += ([0] * (pad_size - seq_len))\n",
    "    else:\n",
    "        token_ids = token_ids[:pad_size]\n",
    "        seq_len = pad_size\n",
    "content\n",
    "token_ids\n",
    "datain = torch.tensor(token_ids)\n",
    "datain = datain.unsqueeze(0)\n",
    "datain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1bc8d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "tensor([[-4.7629,  1.0060]], grad_fn=<AddmmBackward0>)\n",
      "使用PyTorch推理时间： 0.07451820373535156 秒\n",
      "使用PyTorch推理速度： 13.419539788579181 次/秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time_torch = time.time()\n",
    "test_output = model(datain.to(device)) \n",
    "end_time_torch = time.time()\n",
    "\n",
    "inference_time_torch = end_time_torch - start_time_torch\n",
    "inference_speed_torch = 1 / inference_time_torch\n",
    "\n",
    "print(test_output.argmax(dim=1))\n",
    "print(test_output)\n",
    "\n",
    "print(\"使用PyTorch推理时间：\", inference_time_torch, \"秒\")\n",
    "print(\"使用PyTorch推理速度：\", inference_speed_torch, \"次/秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f2d505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(msg):\n",
    "    from openvino.runtime import Core\n",
    "    # 实例化Core对象\n",
    "    core = Core() \n",
    "    det_ov_model = core.read_model('model.xml')\n",
    "        \n",
    "    net = core.compile_model(det_ov_model, device_name=\"AUTO\")\n",
    "    pad_size = 60\n",
    "    with open(\"./vocab.txt\", 'r', encoding='UTF-8') as f:\n",
    "            idx2word = {idx: line.strip() for idx, line in  enumerate(tqdm.tqdm(f))}\n",
    "            word2idx = {idx2word[key]: key for key in  idx2word}\n",
    "    content = str(msg)\n",
    "    token_ids = []\n",
    "    token_ids.append (word2idx['[CLS]'])\n",
    "    for key in content:\n",
    "        token_ids.append(word2idx.get(key, 0))\n",
    "    seq_len = len(token_ids)\n",
    "    mask = []\n",
    "    if pad_size:\n",
    "        if seq_len < pad_size:\n",
    "            token_ids += ([0] * (pad_size - seq_len))\n",
    "        else:\n",
    "            token_ids = token_ids[:pad_size]\n",
    "            seq_len = pad_size\n",
    "    content\n",
    "    token_ids\n",
    "    datain = torch.tensor(token_ids)\n",
    "    datain = datain.unsqueeze(0)\n",
    "    output_node = net.outputs[0] \n",
    "    ir = net.create_infer_request()\n",
    "    outputs = ir.infer(datain)[output_node]\n",
    "    if outputs.argmax() == 1:\n",
    "        return \"这是垃圾内容\"\n",
    "    else:\n",
    "        return \"这是安全内容\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29737819",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Windows\\Temp\\ipykernel_17268\\4126479420.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_infer_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstart_time_openvino\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput_node\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "output_node = net.outputs[0] \n",
    "ir = net.create_infer_request()\n",
    "\n",
    "start_time_openvino = time.time()\n",
    "outputs = ir.infer(datain)[output_node]\n",
    "end_time_openvino = time.time()\n",
    "\n",
    "inference_time_openvino = end_time_openvino - start_time_openvino\n",
    "inference_speed_openvino = 1 / inference_time_openvino\n",
    "\n",
    "print(\"使用OpenVINO推理时间：\", inference_time_openvino, \"秒\")\n",
    "print(\"使用OpenVINO推理速度：\", inference_speed_openvino, \"次/秒\")\n",
    "print(outputs.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeda9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import tqdm\n",
    "import copy\n",
    "import math\n",
    "import tqdm\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from random import *\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():    \n",
    "    return render_template(\"./index.html\")\n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():    \n",
    "    userText = request.args.get('msg')  \n",
    "    response = str(get_response(userText))\n",
    "    return response\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import tqdm\n",
    "import copy\n",
    "import math\n",
    "import tqdm\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from random import *\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # 信息内容检测器\n",
    "    在输入块中键入内容查看结果！\n",
    "    \"\"\")\n",
    "    name = gr.Textbox(label=\"输入\")\n",
    "    output = gr.Textbox(label=\"判断\")\n",
    "    submit_btn = gr.Button(\"提交\")\n",
    "    submit_btn.click(fn=get_response, inputs=name, outputs=output, api_name=\"get_response\")\n",
    "demo.launch()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15abc85d-56d9-4dca-8bc5-4a4d45149c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb93f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
